# torch-blitz
A Quick Torch Recap and Revision via paper implementations in PyTorch
## AlexNet (2012)
### Features
A few highlights from AlexNet are provided below :
1. Used ReLU instead of traditional *tanh* to improve training time. A 25% error rate is achived 6 times faster on a ReLU based Neural Network against a tanh based network.
2. Overlapping Pooling layers used. This resolves issues regarding overfitting.

